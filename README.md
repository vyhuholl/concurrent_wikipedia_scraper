# concurrent_wikipedia_scraper
An exercise in concurrency â€“ a web scraper which makes 20 concurrent requests to random Wikipedia pages, and writes each page's name, url and last modified time to a `CSV` table.
